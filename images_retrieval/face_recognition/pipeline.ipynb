{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Besutodesuka/Retriver_for_verification/blob/main/images_retrieval/face_recognition/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers pillow faiss-cpu torch"
      ],
      "metadata": {
        "id": "1ehXyvoI9HY7",
        "outputId": "8a22748d-c0f0-41b2-db86-e660a92ab563",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import faiss"
      ],
      "metadata": {
        "id": "1DYsWrYj8wKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Hugging Face CLIP model and processor.\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "model = CLIPModel.from_pretrained(model_name)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "LdYFHxVs8rg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embedding(image_path, model, processor):\n",
        "    \"\"\"\n",
        "    Load an image, process it with the CLIP processor,\n",
        "    and return the normalized image embedding.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get image embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model.get_image_features(**inputs)\n",
        "\n",
        "    # Convert to numpy and normalize\n",
        "    embedding = outputs.cpu().numpy()[0]\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    if norm != 0:\n",
        "        embedding = embedding / norm\n",
        "    return embedding\n",
        "\n",
        "def build_face_database(image_folder, model, processor):\n",
        "    \"\"\"\n",
        "    Build a database of embeddings from all images in the folder.\n",
        "    Returns:\n",
        "        - embeddings: numpy array of shape (N, D)\n",
        "        - filenames: list of image filenames corresponding to the embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    filenames = []\n",
        "\n",
        "    for filename in os.listdir(image_folder):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        try:\n",
        "            emb = extract_embedding(image_path, model, processor)\n",
        "            embeddings.append(emb)\n",
        "            filenames.append(filename)\n",
        "            print(f\"Processed {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    embeddings = np.vstack(embeddings).astype('float32')\n",
        "    return embeddings, filenames\n",
        "\n",
        "def create_faiss_index(embeddings):\n",
        "    \"\"\"\n",
        "    Create a FAISS index for normalized embeddings using inner product.\n",
        "    With normalized vectors, inner product is equivalent to cosine similarity.\n",
        "    \"\"\"\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)  # inner product index\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "def verify_face(query_image_path, index, filenames, model, processor, top_k=3, similarity_threshold=0.6):\n",
        "    \"\"\"\n",
        "    Extract the embedding for the query image, search for the top K nearest neighbors,\n",
        "    and compute the average similarity score. If the average similarity exceeds the threshold,\n",
        "    return verification True.\n",
        "    \"\"\"\n",
        "    query_embedding = extract_embedding(query_image_path, model, processor).reshape(1, -1).astype('float32')\n",
        "    # FAISS expects a batch of query vectors\n",
        "    similarities, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # similarities are inner product scores (cosine similarity since embeddings are normalized)\n",
        "    avg_similarity = np.mean(similarities)\n",
        "\n",
        "    # Print details for inspection.\n",
        "    print(\"Top similar images:\")\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        print(f\"{i+1}. {filenames[idx]} - similarity: {similarities[0][i]:.3f}\")\n",
        "\n",
        "    print(f\"Average similarity: {avg_similarity:.3f}\")\n",
        "    verified = avg_similarity >= similarity_threshold\n",
        "    return verified, avg_similarity"
      ],
      "metadata": {
        "id": "U_4E4OIJ8y0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Folder containing known face images (each image should have one face ideally)\n",
        "    known_faces_folder = \"known_faces\"\n",
        "\n",
        "    # Build the face database: embeddings and corresponding filenames.\n",
        "    embeddings, filenames = build_face_database(known_faces_folder, model, processor)\n",
        "\n",
        "    if embeddings.shape[0] == 0:\n",
        "        print(\"No valid embeddings found in the database.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Create the FAISS index from the embeddings.\n",
        "    index = create_faiss_index(embeddings)\n",
        "\n",
        "    # Query image to verify\n",
        "    query_image_path = \"query_face.jpg\"\n",
        "\n",
        "    verified, avg_similarity = verify_face(query_image_path, index, filenames, model, processor,\n",
        "                                           top_k=3, similarity_threshold=0.6)\n",
        "\n",
        "    if verified:\n",
        "        print(\"Face verified!\")\n",
        "    else:\n",
        "        print(\"Face not verified!\")"
      ],
      "metadata": {
        "id": "zzNLCnsl82eC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}